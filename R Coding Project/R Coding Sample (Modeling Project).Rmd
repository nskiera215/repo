---
title: "R Coding Sample Project"
author: "Nick Skiera"
date: "2023-11-14"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(forcats)
library(tibble)
library(ggplot2)
library(ggfortify)
library(glmnet)
library(MASS)
library(caret)
library(class)
library(kernlab)
library(pROC)
```

## Querying Prompts
\smallskip

```{r}
dat <- read.csv("~/Downloads/dataset_2024.csv")
head(dat)
```

1. Which 5 infielders had the quickest exchange times on throws to first base?  

```{r}
infielders <- c(2,4,5,6)
dat %>%
  filter(thrower_position %in% infielders) %>% #filtering out outfielders and pitchers
  filter(exchange_time > 0) %>%
  dplyr::select(thrower_id, thrower_position, exchange_time) %>%
  arrange(exchange_time) %>%
  top_n(-5)
```
The 5 infielders that are not pitchers with the quickest exchange times are throwers 592, 396, 112/159 and 85/687/267/190. I removed pitchers because they are not position players we typically evaluate when looking at fielding. I removed exchange times that are 0 because those are likely glove flips or barehanded players. 

```{r}
infielders_w_pitchers <- c(1,2,4,5,6)
dat %>%
  filter(thrower_position %in% infielders_w_pitchers) %>% #filtering out only outfielders
  filter(exchange_time > 0) %>% 
  dplyr::select(thrower_id, thrower_position, exchange_time) %>%
  arrange(exchange_time) %>%
  top_n(-5)
```

However, if we do include pitchers then the infielders with the quickest exchange times on throws to first are throwers 1, 592, 292, 658 and 396

2. The infield coach wants to see which teams made the most errant throws to first base. An errant throw is described as a throw that bounced and resulted in the runner being safe. Please create a basic visual that you would present to the infield coach to present your findings.  

```{r}
err_throw <- dat %>%
                filter(thrower_position %in% infielders_w_pitchers) %>%
                mutate(errant_throw = as.factor(ifelse(!is.na(bounce_pos_x) & batter_result != "out",1 ,0))) %>%
                filter(errant_throw == 1) %>% #will remove all throws that don't bounce
                arrange(team_id)

team_err_throws <- c()
for(i in 1:15) {
  team_err_throws[i] <- sum(err_throw$team_id == i)
  }

head(df_err <- data.frame(
  team_id = as.character(unique(err_throw$team_id)),
  team_err_throws = team_err_throws) %>%
  mutate(team_order = fct_reorder(team_id, team_err_throws)))

ggplot(df_err, aes(x = team_order, y = team_err_throws)) +
  geom_bar(stat = "identity") +
  labs(title = "Team Errant Throws To First Base") +
  xlab("Team") +
  ylab("Total Errant Throws") +
  coord_flip()
```


3. Looking at all infield throws to first base, given that the distance of the throw to first base was in the top 90th percentile, what team had the best average exchange time? Which team had the largest variation in exchange time on these throws?  

```{r}
dist_throw_1b <- dat %>%
  filter(thrower_position %in% infielders_w_pitchers) %>%
  filter(exchange_time > 0) %>% 
  mutate(distance_of_throw = sqrt((receiver_pos_x - throw_pos_x)^2 + (receiver_pos_y - throw_pos_y)^2)) %>%
  mutate(percentile_rank = ntile(desc(distance_of_throw), 10)) %>%
  arrange(desc(distance_of_throw))

head(throws_90_pctile <- dist_throw_1b %>%
  filter(percentile_rank == 1))

avg_exch_time <- c()
for (i in 1:15) {
  avg_exch_time[i] <- mean(throws_90_pctile$exchange_time[which(throws_90_pctile$team_id == i)])
}
var_exch_time <- c()
for (i in 1:15) {
  var_exch_time[i] <- var(throws_90_pctile$exchange_time[which(throws_90_pctile$team_id == i)])
}

(df_throw_dist <- data.frame(
  team_id = as.character(unique(throws_90_pctile$team_id)),
  avg_exch_time = avg_exch_time,
  var_exch_time = var_exch_time) %>%
  arrange(avg_exch_time))

df_throw_dist$team_id[which(df_throw_dist$avg_exch_time == min(df_throw_dist$avg_exch_time))]
df_throw_dist$team_id[which(df_throw_dist$var_exch_time == max(df_throw_dist$var_exch_time))]
```
Team 2 had the lowest average exchange time at 1.132352 and Team 10 had the largest variation in exchange time at 0.1687950.

4. Given that a throw was made less than 100 feet from first base, is there a correlation between
throw velocity and throw distance? Provide a basic visual alongside a brief explanation.  

```{r, warning = FALSE}
head(short_throw <- dist_throw_1b %>%
  filter(distance_of_throw < 100) %>%
  mutate(throw_velo = sqrt(throw_velo_x^2 + throw_velo_y^2 + throw_velo_z^2)))

cor(short_throw$throw_velo, short_throw$distance_of_throw, use ="complete.obs")

ggplot(short_throw, aes(x = throw_velo, y = distance_of_throw)) +
  geom_point() +
  labs(title = "Scatter Plot of Throw Distance on Throw Velocity") +
  xlab("Velocity of Throw") +
  ylab("Distance of Throw")
```
We can see based on the plot that, outside of a few outliers, there is clearly a strong positive relationship between the distance of the throw and the velocity of the throw with a correlation of 0.7380965. 

\bigskip

## Modeling Project
\smallskip

While often routine, an infielder making a timely and accurate throw to first base is a skill that is critical to the outcome of a game. Arm strength, exchange time, velocity, and the first basemanâ€™s ability to receive an errant throw all determine whether or not an out is made on the play. Using a dataset of throws to first base, I will build a model based on this data that evaluates the talent of a subset of infielders.

```{r}
new_dat <- dat %>%
  filter(thrower_position %in% infielders) %>% #taking out outfielders and pitchers
  subset(thrower_position == fielder_position) %>% #isolates data to just fielders who made throws to 1B
  mutate(thrower_err_throw = as.factor(ifelse(!is.na(bounce_pos_x), 1, 0))) %>% #variable for thrower bouncing a throw to 1B
  mutate(first_base_save = as.factor(ifelse(!is.na(bounce_pos_x) & batter_result == "out", 1, 0))) %>% #1B saves errant throw
  mutate(throw_velo = sqrt(throw_velo_x^2 + throw_velo_y^2 + throw_velo_z^2)) %>% #create overall throw velocity variable
  mutate(num_outs = as.numeric(substr(start_state, 5, 5))) %>% #create variable for number of outs
  mutate(runner_on_third = as.factor(ifelse(substr(start_state, 3, 3) == "3", 1, 0))) %>% #create variable for a runner on third
  mutate(distance_of_throw = sqrt((receiver_pos_x - throw_pos_x)^2 + (receiver_pos_y - throw_pos_y)^2)) %>% #distance of throw
  mutate(batter_result = as.factor(ifelse(batter_result == "out", 1, 0))) %>% #changes batter_result to fct (out = 1, safe = 0)
  remove_rownames() %>% column_to_rownames( var = "throw_id") %>%
  dplyr::select(-c(bounce_pos_x, bounce_pos_y, bounce_velo_x, bounce_velo_y, bounce_velo_z, throw_velo_x, throw_velo_y, throw_velo_z, receiver_position, start_state, end_state, receiver_pos_x, receiver_pos_y, throw_pos_x, throw_pos_y, fielder_id, fielder_position))
new_dat <- na.omit(new_dat)

head(new_dat)
dim(new_dat)
```
I removed outfielders and pitchers because we are evaluating infielders and the main job of pitchers is not fielding baseballs and making good throws to first base. I also removed plays where the thrower didn't field the ball so we can eliminate abnormal plays such as ricochets and double plays involving multiple infielders other than the receiver. I created the variables thrower errant throw, first base save, throw velocity, number of outs, runner on third and distance of throw because I believe that his will add a complexity while also reducing the amount of variables in the data set. I created thrower errant throw because I wanted to eliminate the bounce coordinate variables because they contain NAs. First base save shows us if the first basemen saved the errant throw. I used the velocity coordinates on the throw to create an overall throwing velocity variable which will reduce the dimensionality while maintaining the value of the variables. I wanted to remove the variables start_state and end_state because they are character variables so I extracted the data I deemed important which was number of outs and if there was a runner on third. The number of outs and having a runner on third can add pressure to make a good throw depending on the situation. I also wanted to remove the thrower and receiver coordinates but to maintain their importance I created the distance of throw variable. Finally, I mutated the variable batter_result to be a response factor of whether the batter was thrown out/success (1) or reached base/failure (0). 

```{r}
set.seed(100)

#splitting data into training and test data for cross validation
train_inx <- sample(seq_len(nrow(new_dat)), size = 0.7*nrow(new_dat))
dat_train <- new_dat[train_inx, -c(1, 2, 4)] #removing player ids manually
dat_test <- new_dat[-train_inx,-c(1, 2, 4)]
```

I took out all identification variables (team_id, thrower_id and receiver_id) because these are not predictors. We have 14 predictors. I split the data set into training and test data to use in cross validation and to compare models in-sample and out-of-sample classification accuracy. I hope to find a model with a high classification accuracy in both to have a good fit. A model with a high in-sample classification accuracy and low out-of-sample classification accuracy is over fit because it conforms to the data it was built upon well but not to new data. The opposite scenario means that the model would be under fit because it is able to conform to any data but not the data it was built upon. This is similar to bias-variance trade-off.  

```{r}
set.seed(100)

cv_lasso <- cv.glmnet(y = dat_train$batter_result, x = as.matrix(dat_train[,-9]), alpha = 1, nfolds = 10, family = "binomial")
autoplot(cv_lasso)
```

```{r}
lasso_1se <- glmnet(y = dat_train$batter_result, x = as.matrix(dat_train[,-9]), alpha = 1, lambda = cv_lasso$lambda.1se, family = "binomial")
coef(lasso_1se)

pred_lasso <- predict(lasso_1se, newx = as.matrix(dat_train[,-9]), type = "class")
mean(pred_lasso == dat_train$batter_result)
```
I used cross validation in lasso regression with the desire for dimension reduction and see that it actually eliminated the variable num_outs from the model. I will take num_outs out of the data frame for the rest of the tested models. We now have 13 predictors
```{r}
dat_train1 <- dat_train[,-13]
dat_test1 <- dat_test[,-13]
```


```{r, warning = FALSE}
mod_logit <- glm(batter_result ~ ., data = dat_train1, family = "binomial")
summary(mod_logit)

pred_logit <- predict(mod_logit, type = "response") > 0.5
mean(pred_logit == as.numeric(dat_train1$batter_result))

out_pred_logit <- predict(mod_logit, newdata = dat_test1, type = "response") > 0.5
mean(out_pred_logit == as.numeric(dat_test1$batter_result))
```

We can see that a logistic model is not great with an in-sample accuracy of 0.0218219 and out-of-sample accuracy of 0.02058059. This makes sense as this model is not cross validated. We will move forward to a K-Nearest-Neighbors model using cross validation.

```{r, warning = FALSE}
set.seed(100)

trctrl <- trainControl(method = "cv", number = 10)
knn_fit <- train(batter_result ~ ., data = dat_train1, method = "knn", trControl = trctrl, tuneGrid = expand.grid(k = 1:20))
knn_fit$bestTune

mod_knn <- knn(train = dat_train1[,-9], cl = dat_train1$batter_result, test = dat_train1[,-9], k = knn_fit$bestTune, prob = TRUE)
mean(mod_knn == dat_train1$batter_result)
mean(mod_knn == dat_test1$batter_result)
```

This prediction is a lot better with an in-sample accuracy of 0.9605349 and out-of-sample accuracy of 0.8704615. This model will work but we will test an LDA model to see if we can still improve.

```{r, warning = FALSE}
mod_lda <- lda(batter_result ~., data = dat_train1, prior = rep(1, 2)/2)
pred_lda <- predict(mod_lda, dat_train1)

mean(pred_lda$class == dat_train1$batter_result)
mean(pred_lda$class == dat_test1$batter_result)
```

LDA predicts worse than KNN but still great with an in-sample accuracy of 0.9594206 and out-of-sample accuracy of 0.8699044. Now I will combine all of these models on an in-sample ROC plot and calculate the Areas Under the Curves.

```{r, cache = TRUE}
set.seed(100)

prob_logit <- predict(mod_logit, type = "response")
prob_lass <- predict(lasso_1se, newx = as.matrix(dat_train[,-9]), type = "response")
prob_knn <- 1 - attributes(knn(train = dat_train1[,-9], cl = dat_train1$batter_result, test = dat_train1[,-9], k = knn_fit$bestTune, prob = TRUE))$prob
prob_lda <- predict(mod_lda, dat_train1)$posterior[,2]
df_roc <- data.frame(logit = prob_logit,
                         lasso = prob_lass,
                         knn = prob_knn,
                         lda = prob_lda,
                         batter_result = dat_train$batter_result)
names(df_roc) <- c("logit", "lasso", "knn", "lda", "batter_result" )
rocobj <- roc(batter_result ~ logit + lasso + knn + lda, data = df_roc)
ggroc(rocobj)
df_auc <- data.frame(logit = auc(rocobj$logit),
                     lasso = auc(rocobj$lasso),
                     knn = auc(rocobj$knn),
                     lda = auc(rocobj$lda))
df_auc
```

The logistic model actually has the highest AUC with a 0.9792219. However, due to the low accuracies, I will avoid this model. This might be due to the nature of ROC and how it assigns scores. However, LDA has a great AUC with a 0.9768522 which pairs well with its classification accuracy. 

```{r}
prob_logit_test <- predict(mod_logit, newdata = dat_test1, type = "response")
prob_lass_test <- predict(lasso_1se, newx = as.matrix(dat_test[,-9]), type = "response")
prob_knn_test <- 1 - attributes(knn(train = dat_train1[,-9], cl = dat_train1$batter_result, test = dat_test1[,-9], k = knn_fit$bestTune, prob = TRUE))$prob
prob_lda_test <- predict(mod_lda, dat_test1)$posterior[,2]

df_roc_test <- data.frame(logit = prob_logit_test,
                         lasso = prob_lass_test,
                         knn = prob_knn_test,
                         lda = prob_lda_test,
                         Test = dat_test$batter_result)
names(df_roc_test) <- c("logit", "lasso", "knn", "lda", "batter_result" )
rocobj_test <- roc(batter_result ~ logit + lasso + knn + lda, data = df_roc_test)
ggroc(rocobj_test)
df_auc_test <- data.frame(logit = auc(rocobj_test$logit),
                     lasso = auc(rocobj_test$lasso),
                     knn = auc(rocobj_test$knn),
                     lda = auc(rocobj_test$lda))
df_auc_test
```

We can also see that LDA performs great with an AUC of 0.973346 that pairs well with its out-of-sample classification accuracy

```{r}
mod_lda
```
We can see based on the coefficients of the model that many of the variables actually have a negative coefficient in predicting the result of the batter with a first basement save, velocity of throw, exchange time and having a runner on third having a positive coefficient.

```{r}
plot(pred_lda$x, rep(0, length(pred_lda$x)), col = dat_train1$batter_result, pch = 16, main ="LDA Model Plot", xlab = "Linear Discriminant", ylab = "")
legend("topright", legend = levels(dat_train1$batter_result), col = 1:2, pch = 16)
```
We can see in this plot that there is some overlap between the two results but is overall separated pretty well. This means that the model isn't perfect at predicting the result of a throw to first base which makes sense as baseball is an obscure game that can result in plays like this that should result in an out but don't. Plays that don't result in an out classify more with the negative linear discriminant values and plays that do result in an out correspond more with the positive linear discriminant values.

